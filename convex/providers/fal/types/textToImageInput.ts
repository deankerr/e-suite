/**
 * Generated by orval v6.28.2 üç∫
 * Do not edit manually.
 * FastAPI
 * OpenAPI spec version: 0.1.0
 */
import type { ControlNet } from './controlNet'
import type { Embedding } from './embedding'
import type { IPAdapter } from './iPAdapter'
import type { LoraWeight } from './loraWeight'
import type { TextToImageInputImageFormat } from './textToImageInputImageFormat'
import type { TextToImageInputImageSize } from './textToImageInputImageSize'
import type { TextToImageInputModelArchitecture } from './textToImageInputModelArchitecture'
import type { TextToImageInputScheduler } from './textToImageInputScheduler'

export interface TextToImageInput {
  /**
   * 
            Skips part of the image generation process, leading to slightly different results.
            This means the image renders faster, too.
        
   * @minimum 0
   * @maximum 2
   */
  clip_skip?: number
  /** 
            If set to true, the controlnet will be applied to only the conditional predictions.
         */
  controlnet_guess_mode?: boolean
  /** 
            The control nets to use for the image generation. You can use any number of control nets
            and they will be applied to the image at the specified timesteps.
         */
  controlnets?: ControlNet[]
  /** 
            The embeddings to use for the image generation. Only a single embedding is supported at the moment.
            The embeddings will be used to map the tokens in the prompt to the embedding weights.
         */
  embeddings?: Embedding[]
  /** If set to true, the safety checker will be enabled. */
  enable_safety_checker?: boolean
  /**
   * 
            The CFG (Classifier Free Guidance) scale is a measure of how close you want
            the model to stick to your prompt when looking for a related image to show you.
        
   * @minimum 0
   * @maximum 20
   */
  guidance_scale?: number
  /** The format of the generated image. */
  image_format?: TextToImageInputImageFormat
  /** 
            The size of the generated image. You can choose between some presets or custom height and width
            that **must be multiples of 8**.
         */
  image_size?: TextToImageInputImageSize
  /** 
            The IP adapter to use for the image generation.
         */
  ip_adapter?: IPAdapter[]
  /** 
            The LoRAs to use for the image generation. You can use any number of LoRAs
            and they will be merged together to generate the final image.
         */
  loras?: LoraWeight[]
  /** The architecture of the model to use. If an HF model is used, it will be automatically detected. Otherwise will assume depending on the model name (whether XL is in the name or not). */
  model_architecture?: TextToImageInputModelArchitecture
  /** URL or HuggingFace ID of the base model to generate the image. */
  model_name: string
  /** 
            The negative prompt to use.Use it to address details that you don't want
            in the image. This could be colors, objects, scenery and even the small details
            (e.g. moustache, blurry, low resolution).
         */
  negative_prompt?: string
  /**
   * 
            Number of images to generate in one request. Note that the higher the batch size,
            the longer it will take to generate the images.
        
   * @minimum 1
   * @maximum 8
   */
  num_images?: number
  /**
   * 
            Increasing the amount of steps tells Stable Diffusion that it should take more steps
            to generate your final result which can increase the amount of detail in your image.
        
   * @minimum 0
   * @maximum 150
   */
  num_inference_steps?: number
  /** The prompt to use for generating the image. Be as descriptive as possible for best results. */
  prompt: string
  /** Scheduler / sampler to use for the image denoising process. */
  scheduler?: TextToImageInputScheduler
  /** 
            The same seed and the same prompt given to the same version of Stable Diffusion
            will output the same image every time.
         */
  seed?: number
  /**
   * The size of the tiles to be used for the image generation.
   * @minimum 128
   * @maximum 4096
   */
  tile_height?: number
  /**
   * The stride of the tiles to be used for the image generation.
   * @minimum 64
   * @maximum 2048
   */
  tile_stride_height?: number
  /**
   * The stride of the tiles to be used for the image generation.
   * @minimum 64
   * @maximum 2048
   */
  tile_stride_width?: number
  /**
   * The size of the tiles to be used for the image generation.
   * @minimum 128
   * @maximum 4096
   */
  tile_width?: number
}
