// chat-completion (non-streaming)
// input: target message -> inference parameters, context window/truncation strategy
// output:
//  - text -> message content
//  - token counts (prompt, completion, total), finish reason -> live on job results?
