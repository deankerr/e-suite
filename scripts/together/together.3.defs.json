[
  {
    "id": "austism/chronos-hermes-13b",
    "displayName": "Chronos Hermes (13B)",
    "type": "chat",
    "category": "chat",
    "description": "This model is a 75/25 merge of Chronos (13B) and Nous Hermes (13B) models resulting in having a great ability to produce evocative storywriting and follow a narrative.",
    "descriptionLink": "",
    "link": "",
    "license": "other",
    "creator": "Austism",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "eleutherai/llemma_7b",
    "displayName": "Llemma (7B)",
    "type": "language",
    "category": "language",
    "description": "Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens.",
    "descriptionLink": "",
    "link": "https://huggingface.co/EleutherAI/llemma_7b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "EleutherAI",
    "parameterSize": "6738546688",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "eleutherai/pythia-12b-v0",
    "displayName": "Pythia (12B)",
    "type": "language",
    "category": "language",
    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
    "descriptionLink": "",
    "link": "https://huggingface.co/EleutherAI/pythia-12b-v0",
    "license": "apache-2.0",
    "creator": "EleutherAI",
    "parameterSize": "12000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "eleutherai/pythia-1b-v0",
    "displayName": "Pythia (1B)",
    "type": "language",
    "category": "language",
    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
    "descriptionLink": "",
    "link": "https://huggingface.co/EleutherAI/pythia-1b-v0",
    "license": "apache-2.0",
    "creator": "EleutherAI",
    "parameterSize": "1000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "eleutherai/pythia-2.8b-v0",
    "displayName": "Pythia (2.8B)",
    "type": "language",
    "category": "language",
    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
    "descriptionLink": "",
    "link": "",
    "license": "apache-2.0",
    "creator": "EleutherAI",
    "parameterSize": "2800000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "eleutherai/pythia-6.9b",
    "displayName": "Pythia (6.9B)",
    "type": "language",
    "category": "language",
    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
    "descriptionLink": "",
    "link": "",
    "license": "apache-2.0",
    "creator": "EleutherAI",
    "parameterSize": "6900000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "gryphe/mythomax-l2-13b",
    "displayName": "MythoMax-L2 (13B)",
    "type": "chat",
    "category": "chat",
    "description": "MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model",
    "descriptionLink": "",
    "link": "",
    "license": "other",
    "creator": "Gryphe",
    "parameterSize": "13000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "huggingfaceh4/starchat-alpha",
    "displayName": "StarCoderChat Alpha (16B)",
    "type": "chat",
    "category": "chat",
    "description": "Fine-tuned from StarCoder to act as a helpful coding assistant. As an alpha release is only intended for educational or research purpopses.",
    "descriptionLink": "",
    "link": "https://huggingface.co/HuggingFaceH4/starchat-alpha",
    "license": "bigcode-openrail-m",
    "creator": "HuggingFaceH4",
    "parameterSize": "16000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|system|>\n<|end|>\n<|user|>\n{prompt}<|end|>\n<|assistant|>",
    "stop": [
      "<|endoftext|>",
      "<|end|>"
    ]
  },
  {
    "id": "nousresearch/nous-hermes-13b",
    "displayName": "Nous Hermes (13B)",
    "type": "language",
    "category": "language",
    "description": "LLaMA 13B fine-tuned on over 300,000 instructions. Designed for long responses, low hallucination rate, and absence of censorship mechanisms.",
    "descriptionLink": "",
    "link": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
    "license": "gpl, LLaMA License Agreement (Meta)",
    "creator": "Nous Research",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "nousresearch/nous-hermes-llama2-13b",
    "displayName": "Nous Hermes Llama-2 (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
    "descriptionLink": "",
    "link": "",
    "license": "mit",
    "creator": "Nous Research",
    "parameterSize": "13000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "###",
      "</s>"
    ]
  },
  {
    "id": "nousresearch/nous-hermes-llama2-70b",
    "displayName": "Nous Hermes LLaMA-2 (70B)",
    "type": "chat",
    "category": "chat",
    "description": "Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
    "descriptionLink": "",
    "link": "https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "NousResearch",
    "parameterSize": "70000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n\n### Response:\n",
    "stop": [
      "###",
      "</s>"
    ]
  },
  {
    "id": "nousresearch/nous-hermes-llama-2-7b",
    "displayName": "Nous Hermes LLaMA-2 (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
    "descriptionLink": "",
    "link": "https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "NousResearch",
    "parameterSize": "6738415616",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "###",
      "</s>"
    ]
  },
  {
    "id": "numbersstation/nsql-llama-2-7b",
    "displayName": "NSQL LLaMA-2 (7B)",
    "type": "code",
    "category": "code",
    "description": "NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "Numbers Station",
    "parameterSize": "7000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "open-orca/mistral-7b-openorca",
    "displayName": "OpenOrca Mistral (7B) 8K",
    "type": "chat",
    "category": "chat",
    "description": "An OpenOrca dataset fine-tune on top of Mistral 7B by the OpenOrca team.",
    "descriptionLink": "",
    "link": "https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca",
    "license": "apache-2.0",
    "creator": "OpenOrca",
    "parameterSize": "7241748480",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant",
    "stop": [
      "<|im_end|>"
    ]
  },
  {
    "id": "openassistant/oasst-sft-4-pythia-12b-epoch-3.5",
    "displayName": "Open-Assistant Pythia SFT-4 (12B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",
    "license": "apache-2.0",
    "creator": "LAION",
    "parameterSize": "12000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|prompter|>{prompt}<|endoftext|><|assistant|>",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "openassistant/stablelm-7b-sft-v7-epoch-3",
    "displayName": "Open-Assistant StableLM SFT-7 (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/OpenAssistant/stablelm-7b-sft-v7-epoch-3",
    "license": "cc-by-sa-4.0",
    "creator": "LAION",
    "parameterSize": "7000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|prompter|>{prompt}<|endoftext|><|assistant|>",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "phind/phind-codellama-34b-python-v1",
    "displayName": "Phind Code LLaMA Python v1 (34B)",
    "type": "code",
    "category": "code",
    "description": "This model is fine-tuned from CodeLlama-34B-Python and achieves 69.5% pass@1 on HumanEval.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "Phind",
    "parameterSize": "33743970304",
    "contextLength": "16384",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "</s>",
      "###"
    ]
  },
  {
    "id": "phind/phind-codellama-34b-v2",
    "displayName": "Phind Code LLaMA v2 (34B)",
    "type": "code",
    "category": "code",
    "description": "Phind-CodeLlama-34B-v1 trained on additional 1.5B tokens high-quality programming-related data proficient in Python, C/C++, TypeScript, Java, and more.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "Phind",
    "parameterSize": "33743970304",
    "contextLength": "16384",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\n{prompt}n\n### Assistant\n",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "sg161222/realistic_vision_v3.0_vae",
    "displayName": "Realistic Vision 3.0",
    "type": "image",
    "category": "image",
    "description": "Fine-tune version of Stable Diffusion focused on photorealism.",
    "descriptionLink": "",
    "link": "https://huggingface.co/SG161222/Realistic_Vision_V1.4",
    "license": "creativeml-openrail-m",
    "creator": "SG161222",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "wizardlm/wizardcoder-15b-v1.0",
    "displayName": "WizardCoder v1.0 (15B)",
    "type": "code",
    "category": "code",
    "description": "This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "WizardLM",
    "parameterSize": "15517462528",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n\n### Response:\n",
    "stop": [
      "###",
      "<|endoftext|>"
    ]
  },
  {
    "id": "wizardlm/wizardlm-70b-v1.0",
    "displayName": "WizardLM v1.0 (70B)",
    "type": "language",
    "category": "language",
    "description": "This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "WizardLM",
    "parameterSize": "70000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt} ASSISTANT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "bigcode/starcoder",
    "displayName": "StarCoder (16B)",
    "type": "code",
    "category": "code",
    "description": "Trained on 80+ coding languages, uses Multi Query Attention, an 8K context window, and was trained using the Fill-in-the-Middle objective on 1T tokens.",
    "descriptionLink": "",
    "link": "https://huggingface.co/bigcode/starcoder",
    "license": "bigcode-openrail-m",
    "creator": "BigCode",
    "parameterSize": "16000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>",
      "<|end|>"
    ]
  },
  {
    "id": "databricks/dolly-v2-3b",
    "displayName": "Dolly v2 (3B)",
    "type": "chat",
    "category": "chat",
    "description": "An instruction-following LLM based on pythia-3b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
    "descriptionLink": "",
    "link": "https://huggingface.co/databricks/dolly-v2-3b",
    "license": "mit",
    "creator": "Databricks",
    "parameterSize": "3000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:",
    "stop": [
      "### End"
    ]
  },
  {
    "id": "databricks/dolly-v2-7b",
    "displayName": "Dolly v2 (7B)",
    "type": "chat",
    "category": "chat",
    "description": "An instruction-following LLM based on pythia-7b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
    "descriptionLink": "",
    "link": "https://huggingface.co/databricks/dolly-v2-7b",
    "license": "mit",
    "creator": "Databricks",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:",
    "stop": [
      "### End"
    ]
  },
  {
    "id": "defog/sqlcoder",
    "displayName": "Sqlcoder (15B)",
    "type": "language",
    "category": "language",
    "description": "Defog's SQLCoder is a state-of-the-art LLM for converting natural language questions to SQL queries, fine-tuned from Bigcode's Starcoder 15B model.",
    "descriptionLink": "",
    "link": "",
    "license": "other",
    "creator": "Defog",
    "parameterSize": "15000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instructions:\n\n{prompt}\n\n### Response:\n",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "garage-baind/platypus2-70b-instruct",
    "displayName": "Platypus2 Instruct (70B)",
    "type": "chat",
    "category": "chat",
    "description": "An instruction fine-tuned LLaMA-2 (70B) model by merging Platypus2 (70B) by garage-bAInd and LLaMA-2 Instruct v2 (70B) by upstage.",
    "descriptionLink": "",
    "link": "",
    "license": "CC BY-NC-4.0",
    "creator": "garage-bAInd",
    "parameterSize": "70000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "</s>",
      "###"
    ]
  },
  {
    "id": "huggyllama/llama-13b",
    "displayName": "LLaMA (13B)",
    "type": "language",
    "category": "language",
    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
    "descriptionLink": "",
    "link": "https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "huggyllama/llama-30b",
    "displayName": "LLaMA (30B)",
    "type": "language",
    "category": "language",
    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
    "descriptionLink": "",
    "link": "https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "33000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "huggyllama/llama-65b",
    "displayName": "LLaMA (65B)",
    "type": "language",
    "category": "language",
    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
    "descriptionLink": "",
    "link": "https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "65000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "huggyllama/llama-7b",
    "displayName": "LLaMA (7B)",
    "type": "language",
    "category": "language",
    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
    "descriptionLink": "",
    "link": "https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "lmsys/fastchat-t5-3b-v1.0",
    "displayName": "Vicuna-FastChat-T5 (3B)",
    "type": "chat",
    "category": "chat",
    "description": "Chatbot trained by fine-tuning Flan-t5-xl on user-shared conversations collected from ShareGPT.",
    "descriptionLink": "",
    "link": "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0",
    "license": "apache-2.0",
    "creator": "LM Sys",
    "parameterSize": "3000000000",
    "contextLength": "512",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Human: {prompt}\n### Assistant:",
    "stop": [
      "###",
      "</s>"
    ]
  },
  {
    "id": "lmsys/vicuna-13b-v1.5-16k",
    "displayName": "Vicuna v1.5 16K (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "LM Sys",
    "parameterSize": "13015864320",
    "contextLength": "16384",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt}\nASSISTANT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "lmsys/vicuna-13b-v1.5",
    "displayName": "Vicuna v1.5 (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
    "descriptionLink": "",
    "link": "",
    "license": "llama2",
    "creator": "LM Sys",
    "parameterSize": "13000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt}\nASSISTANT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "lmsys/vicuna-7b-v1.5",
    "displayName": "Vicuna v1.5 (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
    "descriptionLink": "",
    "link": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "LM Sys",
    "parameterSize": "6738415616",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt}\nASSISTANT: Hello!",
    "stop": [
      "</s>",
      "USER:"
    ]
  },
  {
    "id": "mistralai/mistral-7b-instruct-v0.1",
    "displayName": "Mistral (7B) Instruct",
    "type": "chat",
    "category": "chat",
    "description": "instruct fine-tuned version of Mistral-7B-v0.1",
    "descriptionLink": "",
    "link": "",
    "license": "Apache-2",
    "creator": "mistralai",
    "parameterSize": "7241732096",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<s>[INST] {prompt} [/INST]",
    "stop": [
      "[/INST]",
      "</s>"
    ]
  },
  {
    "id": "mistralai/mistral-7b-v0.1",
    "displayName": "Mistral (7B)",
    "type": "language",
    "category": "language",
    "description": "7.3B parameter model that outperforms Llama 2 13B on all benchmarks, approaches CodeLlama 7B performance on code, Uses Grouped-query attention (GQA) for faster inference and Sliding Window Attention (SWA) to handle longer sequences at smaller cost",
    "descriptionLink": "",
    "link": "",
    "license": "Apache-2",
    "creator": "mistralai",
    "parameterSize": "7241732096",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "{prompt}",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "prompthero/openjourney",
    "displayName": "Openjourney v4",
    "type": "image",
    "category": "image",
    "description": "An open source Stable Diffusion model fine tuned model on Midjourney images. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/prompthero/openjourney",
    "license": "creativeml-openrail-m",
    "creator": "Prompt Hero",
    "parameterSize": "13000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "runwayml/stable-diffusion-v1-5",
    "displayName": "Stable Diffusion 1.5",
    "type": "image",
    "category": "image",
    "description": "Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.",
    "descriptionLink": "",
    "link": "https://huggingface.co/runwayml/stable-diffusion-v1-5",
    "license": "creativeml-openrail-m",
    "creator": "Runway ML",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "stabilityai/stable-diffusion-2-1",
    "displayName": "Stable Diffusion 2.1",
    "type": "image",
    "category": "image",
    "description": "Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.",
    "descriptionLink": "",
    "link": "https://huggingface.co/stabilityai/stable-diffusion-2-1",
    "license": "openrail++",
    "creator": "Stability AI",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "stabilityai/stable-diffusion-xl-base-1.0",
    "displayName": "Stable Diffusion XL 1.0",
    "type": "image",
    "category": "image",
    "description": "A text-to-image generative AI model that excels at creating 1024x1024 images.",
    "descriptionLink": "",
    "link": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
    "license": "openrail++",
    "creator": "Stability AI",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "teknium/openhermes-2-mistral-7b",
    "displayName": "OpenHermes-2-Mistral (7B)",
    "type": "chat",
    "category": "chat",
    "description": "State of the art Mistral Fine-tuned on extensive public datasets",
    "descriptionLink": "",
    "link": "",
    "license": "Apache-2",
    "creator": "teknium",
    "parameterSize": "7241732096",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n",
    "stop": [
      "<|im_end|>",
      "<|im_start|>"
    ]
  },
  {
    "id": "togethercomputer/codellama-13b-instruct",
    "displayName": "Code Llama Instruct (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13016028160",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "</s>",
      "[INST]"
    ]
  },
  {
    "id": "togethercomputer/codellama-13b-python",
    "displayName": "Code Llama Python (13B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13016028160",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/codellama-13b",
    "displayName": "Code Llama (13B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13016028160",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/codellama-34b-instruct",
    "displayName": "Code Llama Instruct (34B)",
    "type": "chat",
    "category": "chat",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "34000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "</s>",
      "[INST]"
    ]
  },
  {
    "id": "togethercomputer/codellama-34b-python",
    "displayName": "Code Llama Python (34B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "34000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/codellama-34b",
    "displayName": "Code Llama (34B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "34000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/codellama-7b-instruct",
    "displayName": "Code Llama Instruct (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "6738546688",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "</s>",
      "[INST]"
    ]
  },
  {
    "id": "togethercomputer/codellama-7b-python",
    "displayName": "Code Llama Python (7B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "6738546688",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/codellama-7b",
    "displayName": "Code Llama (7B)",
    "type": "code",
    "category": "code",
    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "LLAMA 2 Community license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "6738546688",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/gpt-jt-6b-v1",
    "displayName": "GPT-JT (6B)",
    "type": "language",
    "category": "language",
    "description": "Fork of GPT-J instruction tuned to excel at few-shot prompts (blog post).",
    "descriptionLink": "https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai",
    "link": "https://huggingface.co/togethercomputer/GPT-JT-6B-v1",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "6700000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/gpt-jt-moderation-6b",
    "displayName": "GPT-JT-Moderation (6B)",
    "type": "language",
    "category": "language",
    "description": "This model can be used to moderate other chatbot models. Built using GPT-JT model fine-tuned on Ontocord.ai's OIG-moderation dataset v0.1.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/GPT-JT-Moderation-6B",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "6700000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/gpt-neoxt-chat-base-20b",
    "displayName": "GPT-NeoXT-Chat-Base (20B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model fine-tuned from EleutherAI’s GPT-NeoX with over 40 million instructions on carbon reduced compute.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "20000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<human>: {prompt}\n<bot>:",
    "stop": [
      "<human>"
    ]
  },
  {
    "id": "togethercomputer/koala-13b",
    "displayName": "Koala (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Chatbot trained by fine-tuning LLaMA on dialogue data gathered from the web.",
    "descriptionLink": "",
    "link": "https://huggingface.co/TheBloke/koala-13B-HF",
    "license": "other",
    "creator": "LM Sys",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt} GPT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/llama-2-7b-32k",
    "displayName": "LLaMA-2-32K (7B)",
    "type": "language",
    "category": "language",
    "description": "Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/LLaMA-2-7B-32K",
    "license": "Meta license",
    "creator": "Together",
    "parameterSize": "6738415616",
    "contextLength": "32768",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "\n\n\n\n",
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/llama-2-7b-32k-instruct",
    "displayName": "LLaMA-2-7B-32K-Instruct (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations, instruction tuned by Together",
    "descriptionLink": "",
    "link": "",
    "license": "Meta license",
    "creator": "Together",
    "parameterSize": "7000000000",
    "contextLength": "32768",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST]\n {prompt} \n[/INST]\n\n",
    "stop": [
      "[INST]",
      "\n\n"
    ]
  },
  {
    "id": "togethercomputer/pythia-chat-base-7b-v0.16",
    "displayName": "Pythia-Chat-Base (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model based on EleutherAI’s Pythia-7B model, and is fine-tuned with data focusing on dialog-style interactions.",
    "descriptionLink": "",
    "link": "",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<human>: {prompt}\n<bot>:",
    "stop": [
      "<human>"
    ]
  },
  {
    "id": "togethercomputer/qwen-7b-chat",
    "displayName": "Qwen-Chat (7B)",
    "type": "chat",
    "category": "chat",
    "description": "7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B-Chat is a large-model-based AI assistant, which is trained with alignment techniques.   ",
    "descriptionLink": "",
    "link": "",
    "license": "Tongyi Qianwen LICENSE AGREEMENT",
    "creator": "Qwen",
    "parameterSize": "7000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n",
    "stop": [
      "<|im_end|>",
      "<|im_start|>"
    ]
  },
  {
    "id": "togethercomputer/qwen-7b",
    "displayName": "Qwen (7B)",
    "type": "language",
    "category": "language",
    "description": "7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. ",
    "descriptionLink": "",
    "link": "",
    "license": "Tongyi Qianwen LICENSE AGREEMENT",
    "creator": "Qwen",
    "parameterSize": "7000000000",
    "contextLength": "8192",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/redpajama-incite-7b-base",
    "displayName": "RedPajama-INCITE (7B)",
    "type": "language",
    "category": "language",
    "description": "Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).",
    "descriptionLink": "https://www.together.xyz/blog/redpajama-models-v1",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "6857302016",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/redpajama-incite-7b-chat",
    "displayName": "RedPajama-INCITE Chat (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-7B-v1 base model.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "6857302016",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<human>: {prompt}\n<bot>:",
    "stop": [
      "<human>"
    ]
  },
  {
    "id": "togethercomputer/redpajama-incite-7b-instruct",
    "displayName": "RedPajama-INCITE Instruct (7B)",
    "type": "language",
    "category": "language",
    "description": "Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-7B-v1 base model.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "6857302016",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/redpajama-incite-base-3b-v1",
    "displayName": "RedPajama-INCITE (3B)",
    "type": "language",
    "category": "language",
    "description": "Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).",
    "descriptionLink": "https://www.together.xyz/blog/redpajama-models-v1",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "2775864320",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/redpajama-incite-chat-3b-v1",
    "displayName": "RedPajama-INCITE Chat (3B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-3B-v1 base model.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "2775864320",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<human>: {prompt}\n<bot>:",
    "stop": [
      "<human>"
    ]
  },
  {
    "id": "togethercomputer/redpajama-incite-instruct-3b-v1",
    "displayName": "RedPajama-INCITE Instruct (3B)",
    "type": "language",
    "category": "language",
    "description": "Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-3B-v1 base model.",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
    "license": "apache-2.0",
    "creator": "Together",
    "parameterSize": "2775864320",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/alpaca-7b",
    "displayName": "Alpaca (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/tatsu-lab/alpaca-7b-wdiff",
    "license": "cc-by-nc-4.0",
    "creator": "Stanford",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "</s>",
      "###"
    ]
  },
  {
    "id": "togethercomputer/codegen2-16b",
    "displayName": "CodeGen2 (16B)",
    "type": "code",
    "category": "code",
    "description": "An autoregressive language models for program synthesis.",
    "descriptionLink": "",
    "link": "https://huggingface.co/Salesforce/codegen2-3_7B",
    "license": "apache-2.0",
    "creator": "Salesforce",
    "parameterSize": "16000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "\n\n"
    ]
  },
  {
    "id": "togethercomputer/codegen2-7b",
    "displayName": "CodeGen2 (7B)",
    "type": "code",
    "category": "code",
    "description": "An autoregressive language models for program synthesis.",
    "descriptionLink": "",
    "link": "https://huggingface.co/Salesforce/codegen2-3_7B",
    "license": "apache-2.0",
    "creator": "Salesforce",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "\n\n"
    ]
  },
  {
    "id": "togethercomputer/falcon-40b-instruct",
    "displayName": "Falcon Instruct (40B)",
    "type": "chat",
    "category": "chat",
    "description": "Falcon-40B-Instruct is a causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/tiiuae/falcon-40b-instruct",
    "license": "apache-2.0",
    "creator": "TII UAE",
    "parameterSize": "40000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "User: {prompt}\nAssistant:",
    "stop": [
      "User:",
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/falcon-40b",
    "displayName": "Falcon (40B)",
    "type": "language",
    "category": "language",
    "description": "Falcon-40B is a causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora.",
    "descriptionLink": "",
    "link": "https://huggingface.co/tiiuae/falcon-40b",
    "license": "apache-2.0",
    "creator": "TII UAE",
    "parameterSize": "40000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/falcon-7b-instruct",
    "displayName": "Falcon Instruct (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Casual decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/tiiuae/falcon-7b-instruct",
    "license": "apache-2.0",
    "creator": "TII UAE",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "User: {prompt}\nAssistant:",
    "stop": [
      "User:",
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/falcon-7b",
    "displayName": "Falcon (7B)",
    "type": "language",
    "category": "language",
    "description": "Causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.",
    "descriptionLink": "",
    "link": "https://huggingface.co/tiiuae/falcon-7b",
    "license": "apache-2.0",
    "creator": "TII UAE",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/guanaco-13b",
    "displayName": "Guanaco (13B) ",
    "type": "chat",
    "category": "chat",
    "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
    "descriptionLink": "",
    "link": "https://huggingface.co/timdettmers/guanaco-33b-merged",
    "license": "apache-2.0, LLaMA License Agreement (Meta)",
    "creator": "Tim Dettmers",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Human: {prompt} ### Assistant:",
    "stop": [
      "###"
    ]
  },
  {
    "id": "togethercomputer/guanaco-65b",
    "displayName": "Guanaco (65B) ",
    "type": "chat",
    "category": "chat",
    "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
    "descriptionLink": "",
    "link": "https://huggingface.co/timdettmers/guanaco-65b-merged",
    "license": "apache-2.0, LLaMA License Agreement (Meta)",
    "creator": "Tim Dettmers",
    "parameterSize": "65000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Human: {prompt} ### Assistant:",
    "stop": [
      "###"
    ]
  },
  {
    "id": "togethercomputer/guanaco-7b",
    "displayName": "Guanaco (7B) ",
    "type": "chat",
    "category": "chat",
    "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/timdettmers/guanaco-7b",
    "license": "apache-2.0, LLaMA License Agreement (Meta)",
    "creator": "Tim Dettmers",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Human: {prompt} ### Assistant:",
    "stop": [
      "###"
    ]
  },
  {
    "id": "togethercomputer/llama-2-13b-chat",
    "displayName": "LLaMA-2 Chat (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-13b-chat",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13015864320",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "[/INST]",
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/llama-2-13b",
    "displayName": "LLaMA-2 (13B)",
    "type": "language",
    "category": "language",
    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-13b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "13015864320",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/llama-2-70b-chat",
    "displayName": "LLaMA-2 Chat (70B)",
    "type": "chat",
    "category": "chat",
    "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-70b-chat",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "68976648192",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "[/INST]",
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/llama-2-70b",
    "displayName": "LLaMA-2 (70B)",
    "type": "language",
    "category": "language",
    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-70b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "68976648192",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/llama-2-7b-chat",
    "displayName": "LLaMA-2 Chat (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-7b-chat",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "6738415616",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "[INST] {prompt} [/INST]",
    "stop": [
      "[/INST]",
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/llama-2-7b",
    "displayName": "LLaMA-2 (7B)",
    "type": "language",
    "category": "language",
    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
    "descriptionLink": "",
    "link": "https://huggingface.co/togethercomputer/llama-2-7b",
    "license": "LLaMA license Agreement (Meta)",
    "creator": "Meta",
    "parameterSize": "6738415616",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/mpt-30b-instruct",
    "displayName": "MPT-Instruct (30B)",
    "type": "language",
    "category": "language",
    "description": "Designed for short-form instruction following, finetuned on Dolly and Anthropic HH-RLHF and other datasets",
    "descriptionLink": "",
    "link": "https://huggingface.co/mosaicml/mpt-30b-instruct",
    "license": "CC-By-SA-3.0",
    "creator": "Mosaic ML",
    "parameterSize": "30000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:\n",
    "stop": [
      "<|endoftext|>",
      "###"
    ]
  },
  {
    "id": "togethercomputer/mpt-30b",
    "displayName": "MPT (30B)",
    "type": "language",
    "category": "language",
    "description": "Decoder-style transformer pretrained from scratch on 1T tokens of English text and code.",
    "descriptionLink": "",
    "link": "https://huggingface.co/mosaicml/mpt-30b",
    "license": "apache-2.0",
    "creator": "Mosaic ML",
    "parameterSize": "30000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/mpt-7b-chat",
    "displayName": "MPT-Chat (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model for dialogue generation finetuned on ShareGPT-Vicuna, Camel-AI, GPTeacher, Guanaco, Baize and some generated datasets.",
    "descriptionLink": "",
    "link": "https://huggingface.co/mosaicml/mpt-7b-chat",
    "license": "cc-by-nc-sa-4.0",
    "creator": "Mosaic ML",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant",
    "stop": [
      "<|im_end|>"
    ]
  },
  {
    "id": "upstage/solar-0-70b-16bit",
    "displayName": "SOLAR v0 (70B)",
    "type": "chat",
    "category": "chat",
    "description": "Language model instruction fine-tuned by upstage.ai on Orca and Alpaca style datasets that reached the top spot in openLLM rankings",
    "descriptionLink": "",
    "link": "",
    "license": "CC BY-NC-4.0",
    "creator": "Upstage",
    "parameterSize": "70000000000",
    "contextLength": "4096",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### System:\nYou are a respectful and helpful assistant.\n### User:\n{prompt}\n### Assistant:",
    "stop": [
      "###"
    ]
  },
  {
    "id": "wavymulder/analog-diffusion",
    "displayName": "Analog Diffusion",
    "type": "image",
    "category": "image",
    "description": "Dreambooth model trained on a diverse set of analog photographs to provide an analog film effect. ",
    "descriptionLink": "",
    "link": "https://huggingface.co/wavymulder/Analog-Diffusion",
    "license": "creativeml-openrail-m",
    "creator": "Wavymulder",
    "parameterSize": "0",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "lmsys/vicuna-13b-v1.3",
    "displayName": "Vicuna v1.3 (13B)",
    "type": "chat",
    "category": "chat",
    "description": "Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Auto-regressive model, based on the transformer architecture.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "LM Sys",
    "parameterSize": "13000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt}\nASSISTANT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "togethercomputer/replit-code-v1-3b",
    "displayName": "Replit-Code-v1 (3B)",
    "type": "code",
    "category": "code",
    "description": "replit-code-v1-3b is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Replit",
    "parameterSize": "3000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/mpt-7b",
    "displayName": "MPT (7B)",
    "type": "",
    "category": "",
    "description": "Decoder-style transformer pretrained from scratch on 1T tokens of English text and code.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Mosaic ML",
    "parameterSize": "7000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/mpt-30b-chat",
    "displayName": "MPT-Chat (30B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat model for dialogue generation finetuned on ShareGPT-Vicuna, Camel-AI, GPTeacher, Guanaco, Baize and some generated datasets.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Mosaic ML",
    "parameterSize": "30000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant",
    "stop": [
      "<|im_end|>"
    ]
  },
  {
    "id": "google/flan-t5-xxl",
    "displayName": "Flan T5 XXL (11B)",
    "type": "",
    "category": "",
    "description": "Flan T5 XXL (11B parameters) is T5 fine-tuned on 1.8K tasks ([paper](https://arxiv.org/pdf/2210.11416.pdf)).",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Google",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "google/flan-t5-xl",
    "displayName": "Flan T5 XL (3B)",
    "type": "",
    "category": "",
    "description": "T5 fine-tuned on more than 1000 additional tasks covering also more languages, making it better than T5 at majority of tasks. ",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Google",
    "parameterSize": "3000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/mpt-7b-instruct",
    "displayName": "MPT-Instruct (7B)",
    "type": "",
    "category": "",
    "description": "Designed for short-form instruction following, finetuned on Dolly and Anthropic HH-RLHF and other datasets",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Mosaic ML",
    "parameterSize": "7000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "numbersstation/nsql-6b",
    "displayName": "NSQL (6B)",
    "type": "",
    "category": "",
    "description": "Foundation model designed specifically for SQL generation tasks. Pre-trained for 3 epochs and fine-tuned for 10 epochs.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Numbers Station",
    "parameterSize": "6000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "togethercomputer/koala-7b",
    "displayName": "Koala (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chatbot trained by fine-tuning LLaMA on dialogue data gathered from the web.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "LM Sys",
    "parameterSize": "7000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt} GPT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "databricks/dolly-v2-12b",
    "displayName": "Dolly v2 (12B)",
    "type": "chat",
    "category": "chat",
    "description": "An instruction-following LLM based on pythia-12b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Databricks",
    "parameterSize": "12000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Instruction:\n{prompt}\n### Response:",
    "stop": [
      "### End"
    ]
  },
  {
    "id": "eleutherai/gpt-neox-20b",
    "displayName": "GPT-NeoX (20B)",
    "type": "",
    "category": "",
    "description": "Autoregressive language model trained on the Pile. Its architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J 6B.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "EleutherAI",
    "parameterSize": "20000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "openassistant/oasst-sft-6-llama-30b-xor",
    "displayName": "Open-Assistant LLaMA SFT-6 (30B)",
    "type": "chat",
    "category": "chat",
    "description": "Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "LAION",
    "parameterSize": "undefined",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "<|prompter|>{prompt}<|endoftext|><|assistant|>",
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "id": "salesforce/instructcodet5p-16b",
    "displayName": "InstructCodeT5 (16B)",
    "type": "chat",
    "category": "chat",
    "description": "Code large language model that can flexibly operate in different modes to support a wide range of code understanding and generation tasks. ",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Salesforce",
    "parameterSize": "33000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "eleutherai/gpt-j-6b",
    "displayName": "GPT-J (6B)",
    "type": "",
    "category": "",
    "description": "Transformer model trained using Ben Wang's Mesh Transformer JAX. ",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "EleutherAI",
    "parameterSize": "6000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "lmsys/vicuna-7b-v1.3",
    "displayName": "Vicuna v1.3 (7B)",
    "type": "chat",
    "category": "chat",
    "description": "Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Auto-regressive model, based on the transformer architecture.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "LM Sys",
    "parameterSize": "7000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "USER: {prompt}\nASSISTANT:",
    "stop": [
      "</s>"
    ]
  },
  {
    "id": "stabilityai/stablelm-base-alpha-3b",
    "displayName": "StableLM-Base-Alpha (3B)",
    "type": "",
    "category": "",
    "description": "Decoder-only language model pre-trained on a diverse collection of English and Code datasets with a sequence length of 4096.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Stability AI",
    "parameterSize": "3000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "stabilityai/stablelm-base-alpha-7b",
    "displayName": "StableLM-Base-Alpha (7B)",
    "type": "",
    "category": "",
    "description": "Decoder-only language model pre-trained on a diverse collection of English and Code datasets with a sequence length of 4096.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Stability AI",
    "parameterSize": "7000000000",
    "contextLength": "undefined",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "",
    "stop": []
  },
  {
    "id": "togethercomputer/guanaco-33b",
    "displayName": "Guanaco (33B) ",
    "type": "chat",
    "category": "chat",
    "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
    "descriptionLink": "",
    "link": "",
    "license": "",
    "creator": "Tim Dettmers",
    "parameterSize": "33000000000",
    "contextLength": "2048",
    "tokenizer": "",
    "instructType": "",
    "promptFormat": "### Human: {prompt} ### Assistant:",
    "stop": [
      "###"
    ]
  }
]