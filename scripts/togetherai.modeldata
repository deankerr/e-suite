[{"name":"Austism/chronos-hermes-13b","display_name":"Chronos Hermes (13B)","display_type":"chat","description":"This model is a 75/25 merge of Chronos (13B) and Nous Hermes (13B) models resulting in having a great ability to produce evocative storywriting and follow a narrative.","license":"other","creator_organization":"Austism","num_parameters":13000000000,"context_length":2048,"config":{"stop":["</s>"],"prompt_format":"### Instruction:\n{prompt}\n### Response:\n"},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"EleutherAI/llemma_7b","display_name":"Llemma (7B)","display_type":"language","description":"Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/EleutherAI/llemma_7b","creator_organization":"EleutherAI","pricing_tier":"Featured","num_parameters":6738546688,"context_length":4096,"config":{},"pricing":{"input":50,"output":50},"descriptionLink":""},{"name":"EleutherAI/pythia-12b-v0","display_name":"Pythia (12B)","display_type":"language","description":"The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.","license":"apache-2.0","link":"https://huggingface.co/EleutherAI/pythia-12b-v0","creator_organization":"EleutherAI","pricing_tier":"supported","num_parameters":12000000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"EleutherAI/pythia-1b-v0","display_name":"Pythia (1B)","display_type":"language","description":"The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.","license":"apache-2.0","link":"https://huggingface.co/EleutherAI/pythia-1b-v0","creator_organization":"EleutherAI","pricing_tier":"supported","num_parameters":1000000000,"context_length":2048,"pricing":{"input":25,"output":25,"hourly":0},"descriptionLink":""},{"name":"EleutherAI/pythia-2.8b-v0","display_name":"Pythia (2.8B)","display_type":"language","description":"The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.","license":"apache-2.0","creator_organization":"EleutherAI","num_parameters":2800000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":25,"output":25,"hourly":0},"link":"","descriptionLink":""},{"name":"EleutherAI/pythia-6.9b","display_name":"Pythia (6.9B)","display_type":"language","description":"The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.","license":"apache-2.0","creator_organization":"EleutherAI","pricing_tier":"featured","num_parameters":6900000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"Gryphe/MythoMax-L2-13b","display_name":"MythoMax-L2 (13B)","display_type":"chat","description":"MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model","license":"other","creator_organization":"Gryphe","num_parameters":13000000000,"release_date":"2023-08-01T00:00:00.000Z","context_length":4096,"config":{"stop":["</s>"],"prompt_format":"### Instruction:\n{prompt}\n### Response:"},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"HuggingFaceH4/starchat-alpha","display_name":"StarCoderChat Alpha (16B)","display_type":"chat","description":"Fine-tuned from StarCoder to act as a helpful coding assistant. As an alpha release is only intended for educational or research purpopses.","license":"bigcode-openrail-m","link":"https://huggingface.co/HuggingFaceH4/starchat-alpha","creator_organization":"HuggingFaceH4","pricing_tier":"supported","num_parameters":16000000000,"context_length":8192,"config":{"stop":["<|endoftext|>","<|end|>"],"prompt_format":"<|system|>\n<|end|>\n<|user|>\n{prompt}<|end|>\n<|assistant|>"},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"NousResearch/Nous-Hermes-13b","display_name":"Nous Hermes (13B)","display_type":"language","description":"LLaMA 13B fine-tuned on over 300,000 instructions. Designed for long responses, low hallucination rate, and absence of censorship mechanisms.","license":"gpl, LLaMA License Agreement (Meta)","link":"https://huggingface.co/NousResearch/Nous-Hermes-13b","creator_organization":"Nous Research","pricing_tier":"supported","num_parameters":13000000000,"context_length":2048,"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"NousResearch/Nous-Hermes-Llama2-13b","display_name":"Nous Hermes Llama-2 (13B)","display_type":"chat","description":"Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions.","license":"mit","creator_organization":"Nous Research","pricing_tier":"featured","num_parameters":13000000000,"context_length":4096,"config":{"prompt_format":"### Instruction:\n{prompt}\n### Response:\n","stop":["###","</s>"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"NousResearch/Nous-Hermes-Llama2-70b","display_name":"Nous Hermes LLaMA-2 (70B)","display_type":"chat","description":"Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 300,000 instructions.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b","creator_organization":"NousResearch","pricing_tier":"Featured","num_parameters":70000000000,"context_length":4096,"config":{"stop":["###","</s>"],"prompt_format":"### Instruction:\n{prompt}\n\n### Response:\n"},"pricing":{"input":250,"output":250},"descriptionLink":""},{"name":"NousResearch/Nous-Hermes-llama-2-7b","display_name":"Nous Hermes LLaMA-2 (7B)","display_type":"chat","description":"Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 300,000 instructions.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b","creator_organization":"NousResearch","pricing_tier":"Featured","num_parameters":6738415616,"context_length":4096,"config":{"prompt_format":"### Instruction:\n{prompt}\n### Response:\n","stop":["###","</s>"]},"pricing":{"input":50,"output":50},"descriptionLink":""},{"name":"NumbersStation/nsql-llama-2-7B","display_name":"NSQL LLaMA-2 (7B)","display_type":"code","description":"NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks.","license":"llama2","creator_organization":"Numbers Station","pricing_tier":"supported","num_parameters":7000000000,"context_length":4096,"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"Open-Orca/Mistral-7B-OpenOrca","display_name":"OpenOrca Mistral (7B) 8K","display_type":"chat","description":"An OpenOrca dataset fine-tune on top of Mistral 7B by the OpenOrca team.","license":"apache-2.0","link":"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca","creator_organization":"OpenOrca","pricing_tier":"Featured","num_parameters":7241748480,"context_length":8192,"config":{"stop":["<|im_end|>"],"prompt_format":"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"},"pricing":{"input":50,"output":50},"descriptionLink":""},{"name":"OpenAssistant/llama2-70b-oasst-sft-v10","display_name":"LLaMA 2 SFT v10 (70B)","display_type":"chat","description":"An Open-Assistant fine-tuned model from LLaMA-2 70B.","license":"llama2","creator_organization":"OpenAssistant","pricing_tier":"supported","num_parameters":70000000000,"context_length":4096,"config":{"stop":["</s>","<|im_end|>"],"prompt_format":"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"},"pricing":{"input":250,"output":250,"hourly":0},"link":"","descriptionLink":""},{"name":"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5","display_name":"Open-Assistant Pythia SFT-4 (12B)","display_type":"chat","description":"Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ","license":"apache-2.0","link":"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5","creator_organization":"LAION","pricing_tier":"supported","num_parameters":12000000000,"context_length":2048,"config":{"stop":["<|endoftext|>"],"prompt_format":"<|prompter|>{prompt}<|endoftext|><|assistant|>"},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"OpenAssistant/stablelm-7b-sft-v7-epoch-3","display_name":"Open-Assistant StableLM SFT-7 (7B)","display_type":"chat","description":"Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ","license":"cc-by-sa-4.0","link":"https://huggingface.co/OpenAssistant/stablelm-7b-sft-v7-epoch-3","creator_organization":"LAION","pricing_tier":"supported","num_parameters":7000000000,"context_length":4096,"config":{"stop":["<|endoftext|>"],"prompt_format":"<|prompter|>{prompt}<|endoftext|><|assistant|>"},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"Phind/Phind-CodeLlama-34B-Python-v1","display_name":"Phind Code LLaMA Python v1 (34B)","display_type":"code","description":"This model is fine-tuned from CodeLlama-34B-Python and achieves 69.5% pass@1 on HumanEval.","license":"llama2","creator_organization":"Phind","pricing_tier":"supported","num_parameters":33743970304,"context_length":16384,"config":{"prompt_format":"### Instruction:\n{prompt}\n### Response:\n","stop":["</s>","###"]},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"Phind/Phind-CodeLlama-34B-v2","display_name":"Phind Code LLaMA v2 (34B)","display_type":"code","description":"Phind-CodeLlama-34B-v1 trained on additional 1.5B tokens high-quality programming-related data proficient in Python, C/C++, TypeScript, Java, and more.","license":"llama2","creator_organization":"Phind","pricing_tier":"supported","num_parameters":33743970304,"context_length":16384,"config":{"prompt_format":"### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\n{prompt}n\n### Assistant\n","stop":["</s>"]},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"SG161222/Realistic_Vision_V3.0_VAE","display_name":"Realistic Vision 3.0","display_type":"image","description":"Fine-tune version of Stable Diffusion focused on photorealism.","license":"creativeml-openrail-m","link":"https://huggingface.co/SG161222/Realistic_Vision_V1.4","creator_organization":"SG161222","pricing_tier":"supported","external_pricing_url":"https://www.together.xyz/apis#pricing","config":{"height":1024,"width":1024,"steps":20,"number_of_images":2,"seed":42},"descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}},{"name":"WizardLM/WizardCoder-15B-V1.0","display_name":"WizardCoder v1.0 (15B)","display_type":"code","description":"This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.","license":"llama2","creator_organization":"WizardLM","pricing_tier":"supported","num_parameters":15517462528,"context_length":8192,"config":{"prompt_format":"### Instruction:\n{prompt}\n\n### Response:\n","stop":["###","<|endoftext|>"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"WizardLM/WizardCoder-Python-34B-V1.0","display_name":"WizardCoder Python v1.0 (34B)","display_type":"code","description":"This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.","license":"llama2","creator_organization":"WizardLM","pricing_tier":"supported","num_parameters":34000000000,"context_length":8192,"config":{"stop":["</s>","###"],"prompt_format":"### Instruction:\n{prompt}\n### Response:\n"},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"WizardLM/WizardLM-70B-V1.0","display_name":"WizardLM v1.0 (70B)","display_type":"language","description":"This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities.","license":"llama2","creator_organization":"WizardLM","pricing_tier":"supported","num_parameters":70000000000,"context_length":4096,"config":{"stop":["</s>"],"prompt_format":"USER: {prompt} ASSISTANT:"},"pricing":{"input":250,"output":250,"hourly":0},"link":"","descriptionLink":""},{"name":"bigcode/starcoder","display_name":"StarCoder (16B)","display_type":"code","description":"Trained on 80+ coding languages, uses Multi Query Attention, an 8K context window, and was trained using the Fill-in-the-Middle objective on 1T tokens.","license":"bigcode-openrail-m","link":"https://huggingface.co/bigcode/starcoder","creator_organization":"BigCode","pricing_tier":"supported","num_parameters":16000000000,"context_length":8192,"config":{"stop":["<|endoftext|>","<|end|>"]},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"databricks/dolly-v2-12b","display_name":"Dolly v2 (12B)","display_type":"chat","description":"An instruction-following LLM based on pythia-12b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.","license":"mit","link":"https://huggingface.co/databricks/dolly-v2-12b","creator_organization":"Databricks","pricing_tier":"supported","num_parameters":12000000000,"context_length":2048,"config":{"stop":["### End"],"prompt_format":"### Instruction:\n{prompt}\n### Response:"},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"databricks/dolly-v2-3b","display_name":"Dolly v2 (3B)","display_type":"chat","description":"An instruction-following LLM based on pythia-3b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.","license":"mit","link":"https://huggingface.co/databricks/dolly-v2-3b","creator_organization":"Databricks","pricing_tier":"supported","num_parameters":3000000000,"context_length":2048,"config":{"stop":["### End"],"prompt_format":"### Instruction:\n{prompt}\n### Response:"},"pricing":{"input":25,"output":25,"hourly":0},"descriptionLink":""},{"name":"databricks/dolly-v2-7b","display_name":"Dolly v2 (7B)","display_type":"chat","description":"An instruction-following LLM based on pythia-7b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.","license":"mit","link":"https://huggingface.co/databricks/dolly-v2-7b","creator_organization":"Databricks","pricing_tier":"featured","num_parameters":7000000000,"context_length":2048,"config":{"stop":["### End"],"prompt_format":"### Instruction:\n{prompt}\n### Response:"},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"defog/sqlcoder","display_name":"Sqlcoder (15B)","display_type":"language","description":"Defog's SQLCoder is a state-of-the-art LLM for converting natural language questions to SQL queries, fine-tuned from Bigcode's Starcoder 15B model.","license":"other","creator_organization":"Defog","pricing_tier":"supported","num_parameters":15000000000,"context_length":8192,"config":{"stop":["<|endoftext|>"],"prompt_format":"### Instructions:\n\n{prompt}\n\n### Response:\n"},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"garage-bAInd/Platypus2-70B-instruct","display_name":"Platypus2 Instruct (70B)","display_type":"chat","description":"An instruction fine-tuned LLaMA-2 (70B) model by merging Platypus2 (70B) by garage-bAInd and LLaMA-2 Instruct v2 (70B) by upstage.","license":"CC BY-NC-4.0","creator_organization":"garage-bAInd","pricing_tier":"featured","num_parameters":70000000000,"context_length":4096,"config":{"stop":["</s>","###"],"prompt_format":"### Instruction:\n{prompt}\n### Response:\n"},"pricing":{"input":250,"output":250,"hourly":0},"link":"","descriptionLink":""},{"name":"huggyllama/llama-13b","display_name":"LLaMA (13B)","display_type":"language","description":"An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129","creator_organization":"Meta","pricing_tier":"supported","num_parameters":13000000000,"context_length":2048,"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"huggyllama/llama-30b","display_name":"LLaMA (30B)","display_type":"language","description":"An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129","creator_organization":"Meta","num_parameters":33000000000,"context_length":2048,"pricing":{"input":200,"output":200,"hourly":0},"descriptionLink":""},{"name":"huggyllama/llama-65b","display_name":"LLaMA (65B)","display_type":"language","description":"An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129","creator_organization":"Meta","pricing_tier":"supported","num_parameters":65000000000,"context_length":2048,"pricing":{"input":250,"output":250,"hourly":0},"descriptionLink":""},{"name":"huggyllama/llama-7b","display_name":"LLaMA (7B)","display_type":"language","description":"An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/decapoda-research/llama-30b-hf-int4/commit/95d097b272bd0a84a164aa8116e8c09661487581#d2h-740129","creator_organization":"Meta","pricing_tier":"supported","num_parameters":7000000000,"context_length":2048,"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"lmsys/fastchat-t5-3b-v1.0","display_name":"Vicuna-FastChat-T5 (3B)","display_type":"chat","description":"Chatbot trained by fine-tuning Flan-t5-xl on user-shared conversations collected from ShareGPT.","license":"apache-2.0","link":"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0","creator_organization":"LM Sys","pricing_tier":"featured","num_parameters":3000000000,"context_length":512,"config":{"stop":["###","</s>"],"prompt_format":"### Human: {prompt}\n### Assistant:"},"pricing":{"input":25,"output":25,"hourly":0},"descriptionLink":""},{"name":"lmsys/vicuna-13b-v1.5-16k","display_name":"Vicuna v1.5 16K (13B)","display_type":"chat","description":"Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.","license":"llama2","creator_organization":"LM Sys","pricing_tier":"supported","num_parameters":13015864320,"context_length":16384,"config":{"prompt_format":"USER: {prompt}\nASSISTANT:","stop":["</s>"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"lmsys/vicuna-13b-v1.5","display_name":"Vicuna v1.5 (13B)","display_type":"chat","description":"Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.","license":"llama2","creator_organization":"LM Sys","pricing_tier":"supported","num_parameters":13000000000,"context_length":4096,"config":{"stop":["</s>"],"prompt_format":"USER: {prompt}\nASSISTANT:"},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"lmsys/vicuna-7b-v1.5","display_name":"Vicuna v1.5 (7B)","display_type":"chat","description":"Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/lmsys/vicuna-7b-v1.3","creator_organization":"LM Sys","pricing_tier":"Featured","num_parameters":6738415616,"context_length":4096,"config":{"stop":["</s>","USER:"],"prompt_format":"USER: {prompt}\nASSISTANT: Hello!"},"descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}},{"name":"mistralai/Mistral-7B-Instruct-v0.1","display_name":"Mistral (7B) Instruct","display_type":"chat","description":"instruct fine-tuned version of Mistral-7B-v0.1","license":"Apache-2","creator_organization":"mistralai","num_parameters":7241732096,"release_date":"2023-09-27T00:00:00.000Z","context_length":4096,"config":{"stop":["[/INST]","</s>"],"prompt_format":"<s>[INST] {prompt} [/INST]"},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"mistralai/Mistral-7B-v0.1","display_name":"Mistral (7B)","display_type":"language","description":"7.3B parameter model that outperforms Llama 2 13B on all benchmarks, approaches CodeLlama 7B performance on code, Uses Grouped-query attention (GQA) for faster inference and Sliding Window Attention (SWA) to handle longer sequences at smaller cost","license":"Apache-2","creator_organization":"mistralai","num_parameters":7241732096,"release_date":"2023-09-27T00:00:00.000Z","context_length":4096,"config":{"stop":["</s>"],"prompt_format":"{prompt}"},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"prompthero/openjourney","display_name":"Openjourney v4","display_type":"image","description":"An open source Stable Diffusion model fine tuned model on Midjourney images. ","license":"creativeml-openrail-m","link":"https://huggingface.co/prompthero/openjourney","creator_organization":"Prompt Hero","pricing_tier":"featured","num_parameters":13000000000,"external_pricing_url":"https://www.together.xyz/apis#pricing","config":{"height":512,"width":512,"steps":20,"number_of_images":2,"seed":42},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"runwayml/stable-diffusion-v1-5","display_name":"Stable Diffusion 1.5","display_type":"image","description":"Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.","license":"creativeml-openrail-m","link":"https://huggingface.co/runwayml/stable-diffusion-v1-5","creator_organization":"Runway ML","pricing_tier":"featured","external_pricing_url":"https://www.together.xyz/apis#pricing","config":{"height":512,"width":512,"steps":20,"number_of_images":2,"seed":42},"descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}},{"name":"stabilityai/stable-diffusion-2-1","display_name":"Stable Diffusion 2.1","display_type":"image","description":"Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.","license":"openrail++","link":"https://huggingface.co/stabilityai/stable-diffusion-2-1","creator_organization":"Stability AI","pricing_tier":"featured","external_pricing_url":"https://www.together.xyz/apis#pricing","descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}},{"name":"stabilityai/stable-diffusion-xl-base-1.0","display_name":"Stable Diffusion XL 1.0","display_type":"image","description":"A text-to-image generative AI model that excels at creating 1024x1024 images.","license":"openrail++","link":"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0","creator_organization":"Stability AI","pricing_tier":"featured","external_pricing_url":"https://www.together.xyz/apis#pricing","config":{"height":1024,"width":1024,"steps":20,"number_of_images":2,"seed":42},"descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}},{"name":"teknium/OpenHermes-2-Mistral-7B","display_name":"OpenHermes-2-Mistral (7B)","display_type":"chat","description":"State of the art Mistral Fine-tuned on extensive public datasets","license":"Apache-2","creator_organization":"teknium","pricing_tier":"Featured","num_parameters":7241732096,"release_date":"2023-10-27T00:00:00.000Z","config":{"stop":["<|im_end|>","<|im_start|>"],"prompt_format":"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n","pre_prompt":"<|im_start|>system\nYou are thoughtful, helpful, polite, honest, and friendly<|im_end|>\n"},"pricing":{"input":50,"output":50},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-13b-Instruct","display_name":"Code Llama Instruct (13B)","display_type":"chat","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"13016028160","finetuning_supported":true,"context_length":8192,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["</s>","[INST]"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-13b-Python","display_name":"Code Llama Python (13B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"13016028160","finetuning_supported":true,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-13b","display_name":"Code Llama (13B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"13016028160","finetuning_supported":true,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":100,"output":100,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-34b-Instruct","display_name":"Code Llama Instruct (34B)","display_type":"chat","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":34000000000,"context_length":8192,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["</s>","[INST]"]},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-34b-Python","display_name":"Code Llama Python (34B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":34000000000,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-34b","display_name":"Code Llama (34B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":34000000000,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":200,"output":200,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-7b-Instruct","display_name":"Code Llama Instruct (7B)","display_type":"chat","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"6738546688","finetuning_supported":true,"context_length":8192,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["</s>","[INST]"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-7b-Python","display_name":"Code Llama Python (7B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"6738546688","finetuning_supported":true,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/CodeLlama-7b","display_name":"Code Llama (7B)","display_type":"code","description":"Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","license":"LLAMA 2 Community license Agreement (Meta)","creator_organization":"Meta","num_parameters":"6738546688","finetuning_supported":true,"context_length":8192,"config":{"stop":["</s>"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/GPT-JT-6B-v1","display_name":"GPT-JT (6B)","display_type":"language","description":"Fork of GPT-J instruction tuned to excel at few-shot prompts (blog post).","descriptionLink":"https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/GPT-JT-6B-v1","creator_organization":"Together","pricing_tier":"featured","num_parameters":6700000000,"release_date":"2022-11-29T00:00:00.000Z","context_length":2048,"pricing":{"input":50,"output":50,"hourly":0}},{"name":"togethercomputer/GPT-JT-Moderation-6B","display_name":"GPT-JT-Moderation (6B)","display_type":"language","description":"This model can be used to moderate other chatbot models. Built using GPT-JT model fine-tuned on Ontocord.ai's OIG-moderation dataset v0.1.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/GPT-JT-Moderation-6B","creator_organization":"Together","pricing_tier":"featured","num_parameters":6700000000,"context_length":2048,"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/GPT-NeoXT-Chat-Base-20B","display_name":"GPT-NeoXT-Chat-Base (20B)","display_type":"chat","description":"Chat model fine-tuned from EleutherAI's GPT-NeoX with over 40 million instructions on carbon reduced compute.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B","creator_organization":"Together","pricing_tier":"featured","num_parameters":20000000000,"context_length":2048,"config":{"prompt_format":"<human>: {prompt}\n<bot>:","stop":["<human>"]},"max_tokens":995,"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/Koala-13B","display_name":"Koala (13B)","display_type":"chat","description":"Chatbot trained by fine-tuning LLaMA on dialogue data gathered from the web.","license":"other","link":"https://huggingface.co/TheBloke/koala-13B-HF","creator_organization":"LM Sys","pricing_tier":"supported","num_parameters":13000000000,"context_length":2048,"config":{"stop":["</s>"],"prompt_format":"USER: {prompt} GPT:"},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/LLaMA-2-7B-32K","display_name":"LLaMA-2-32K (7B)","display_type":"language","description":"Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations.","license":"Meta license","link":"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K","creator_organization":"Together","pricing_tier":"supported","num_parameters":"6738415616","finetuning_supported":true,"context_length":32768,"config":{"stop":["\n\n\n\n","<|endoftext|>"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/Llama-2-7B-32K-Instruct","display_name":"LLaMA-2-7B-32K-Instruct (7B)","display_type":"chat","description":"Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations, instruction tuned by Together","license":"Meta license","creator_organization":"Together","pricing_tier":"supported","num_parameters":7000000000,"finetuning_supported":true,"context_length":32768,"config":{"prompt_format":"[INST]\n {prompt} \n[/INST]\n\n","stop":["[INST]","\n\n"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/Pythia-Chat-Base-7B-v0.16","display_name":"Pythia-Chat-Base (7B)","display_type":"chat","description":"Chat model based on EleutherAI's Pythia-7B model, and is fine-tuned with data focusing on dialog-style interactions.","license":"apache-2.0","creator_organization":"Together","pricing_tier":"featured","num_parameters":7000000000,"finetuning_supported":true,"context_length":2048,"config":{"prompt_format":"<human>: {prompt}\n<bot>:","stop":["<human>"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/Qwen-7B-Chat","display_name":"Qwen-Chat (7B)","display_type":"chat","description":"7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B-Chat is a large-model-based AI assistant, which is trained with alignment techniques.","license":"Tongyi Qianwen LICENSE AGREEMENT","creator_organization":"Qwen","num_parameters":7000000000,"release_date":"2023-08-01T00:00:00.000Z","context_length":8192,"config":{"stop":["<|im_end|>","<|im_start|>"],"prompt_format":"\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/Qwen-7B","display_name":"Qwen (7B)","display_type":"language","description":"7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc.","license":"Tongyi Qianwen LICENSE AGREEMENT","creator_organization":"Qwen","num_parameters":7000000000,"release_date":"2023-08-01T00:00:00.000Z","context_length":8192,"config":{"stop":["<|im_end|>","<|endoftext|>"]},"pricing":{"input":50,"output":50,"hourly":0},"link":"","descriptionLink":""},{"name":"togethercomputer/RedPajama-INCITE-7B-Base","display_name":"RedPajama-INCITE (7B)","display_type":"language","description":"Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).","descriptionLink":"https://www.together.xyz/blog/redpajama-models-v1","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base","creator_organization":"Together","pricing_tier":"featured","num_parameters":"6857302016","finetuning_supported":true,"context_length":2048,"pricing":{"input":50,"output":50,"hourly":0}},{"name":"togethercomputer/RedPajama-INCITE-7B-Chat","display_name":"RedPajama-INCITE Chat (7B)","display_type":"chat","description":"Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-7B-v1 base model.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat","creator_organization":"Together","pricing_tier":"featured","num_parameters":"6857302016","finetuning_supported":true,"context_length":2048,"config":{"prompt_format":"<human>: {prompt}\n<bot>:","stop":["<human>"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/RedPajama-INCITE-7B-Instruct","display_name":"RedPajama-INCITE Instruct (7B)","display_type":"language","description":"Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-7B-v1 base model.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct","creator_organization":"Together","pricing_tier":"featured","num_parameters":"6857302016","finetuning_supported":true,"context_length":2048,"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/RedPajama-INCITE-Base-3B-v1","display_name":"RedPajama-INCITE (3B)","display_type":"language","description":"Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).","descriptionLink":"https://www.together.xyz/blog/redpajama-models-v1","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1","creator_organization":"Together","pricing_tier":"featured","num_parameters":"2775864320","finetuning_supported":true,"context_length":2048,"pricing":{"input":25,"output":25,"hourly":0}},{"name":"togethercomputer/RedPajama-INCITE-Chat-3B-v1","display_name":"RedPajama-INCITE Chat (3B)","display_type":"chat","description":"Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-3B-v1 base model.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1","creator_organization":"Together","pricing_tier":"featured","num_parameters":"2775864320","finetuning_supported":true,"context_length":2048,"config":{"prompt_format":"<human>: {prompt}\n<bot>:","stop":["<human>"]},"pricing":{"input":25,"output":25,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/RedPajama-INCITE-Instruct-3B-v1","display_name":"RedPajama-INCITE Instruct (3B)","display_type":"language","description":"Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-3B-v1 base model.","license":"apache-2.0","link":"https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1","creator_organization":"Together","pricing_tier":"featured","num_parameters":"2775864320","finetuning_supported":true,"context_length":2048,"pricing":{"input":25,"output":25,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/alpaca-7b","display_name":"Alpaca (7B)","display_type":"chat","description":"Fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. ","license":"cc-by-nc-4.0","link":"https://huggingface.co/tatsu-lab/alpaca-7b-wdiff","creator_organization":"Stanford","pricing_tier":"supported","num_parameters":7000000000,"context_length":2048,"config":{"stop":["</s>","###"],"prompt_format":"### Instruction:\n{prompt}\n### Response:\n"},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/codegen2-16B","display_name":"CodeGen2 (16B)","display_type":"code","description":"An autoregressive language models for program synthesis.","license":"apache-2.0","link":"https://huggingface.co/Salesforce/codegen2-3_7B","creator_organization":"Salesforce","pricing_tier":"supported","num_parameters":16000000000,"release_date":"2022-03-25T00:00:00.000Z","context_length":2048,"config":{"stop":["\n\n"]},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/codegen2-7B","display_name":"CodeGen2 (7B)","display_type":"code","description":"An autoregressive language models for program synthesis.","license":"apache-2.0","link":"https://huggingface.co/Salesforce/codegen2-3_7B","creator_organization":"Salesforce","pricing_tier":"supported","num_parameters":7000000000,"release_date":"2022-03-25T00:00:00.000Z","context_length":2048,"config":{"stop":["\n\n"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/falcon-40b-instruct","display_name":"Falcon Instruct (40B)","display_type":"chat","description":"Falcon-40B-Instruct is a causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. ","license":"apache-2.0","link":"https://huggingface.co/tiiuae/falcon-40b-instruct","creator_organization":"TII UAE","pricing_tier":"supported","num_parameters":40000000000,"context_length":2048,"config":{"prompt_format":"User: {prompt}\nAssistant:","stop":["User:","</s>"]},"pricing":{"input":200,"output":200,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/falcon-40b","display_name":"Falcon (40B)","display_type":"language","description":"Falcon-40B is a causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora.","license":"apache-2.0","link":"https://huggingface.co/tiiuae/falcon-40b","creator_organization":"TII UAE","pricing_tier":"supported","num_parameters":40000000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":200,"output":200,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/falcon-7b-instruct","display_name":"Falcon Instruct (7B)","display_type":"chat","description":"Casual decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. ","license":"apache-2.0","link":"https://huggingface.co/tiiuae/falcon-7b-instruct","creator_organization":"TII UAE","pricing_tier":"featured","num_parameters":7000000000,"context_length":2048,"config":{"prompt_format":"User: {prompt}\nAssistant:","stop":["User:","</s>"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/falcon-7b","display_name":"Falcon (7B)","display_type":"language","description":"Causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.","license":"apache-2.0","link":"https://huggingface.co/tiiuae/falcon-7b","creator_organization":"TII UAE","pricing_tier":"featured","num_parameters":7000000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/guanaco-13b","display_name":"Guanaco (13B) ","display_type":"chat","description":"Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.","license":"apache-2.0, LLaMA License Agreement (Meta)","link":"https://huggingface.co/timdettmers/guanaco-33b-merged","creator_organization":"Tim Dettmers","pricing_tier":"Supported","num_parameters":13000000000,"context_length":2048,"config":{"stop":["###"],"prompt_format":"### Human: {prompt} ### Assistant:"},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/guanaco-65b","display_name":"Guanaco (65B) ","display_type":"chat","description":"Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.","license":"apache-2.0, LLaMA License Agreement (Meta)","link":"https://huggingface.co/timdettmers/guanaco-65b-merged","creator_organization":"Tim Dettmers","pricing_tier":"Supported","num_parameters":65000000000,"context_length":2048,"config":{"stop":["###"],"prompt_format":"### Human: {prompt} ### Assistant:"},"pricing":{"input":250,"output":250,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/guanaco-7b","display_name":"Guanaco (7B) ","display_type":"chat","description":"Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks. ","license":"apache-2.0, LLaMA License Agreement (Meta)","link":"https://huggingface.co/timdettmers/guanaco-7b","creator_organization":"Tim Dettmers","num_parameters":7000000000,"context_length":2048,"config":{"stop":["###"],"prompt_format":"### Human: {prompt} ### Assistant:"},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/llama-2-13b-chat","display_name":"LLaMA-2 Chat (13B)","display_type":"chat","description":"Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-13b-chat","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"13015864320","finetuning_supported":true,"context_length":4096,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["[/INST]","</s>"]},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/llama-2-13b","display_name":"LLaMA-2 (13B)","display_type":"language","description":"Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-13b","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"13015864320","finetuning_supported":true,"context_length":4096,"config":{},"pricing":{"input":100,"output":100,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/llama-2-70b-chat","display_name":"LLaMA-2 Chat (70B)","display_type":"chat","description":"Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-70b-chat","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"68976648192","finetuning_supported":true,"context_length":4096,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["[/INST]","</s>"]},"pricing":{"input":250,"output":250,"hourly":0},"autopilot_pool":"cr-a100-80-2x","descriptionLink":""},{"name":"togethercomputer/llama-2-70b","display_name":"LLaMA-2 (70B)","display_type":"language","description":"Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-70b","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"68976648192","finetuning_supported":true,"context_length":4096,"config":{},"pricing":{"input":250,"output":250,"hourly":0},"autopilot_pool":"cr-a100-80-2x","descriptionLink":""},{"name":"togethercomputer/llama-2-7b-chat","display_name":"LLaMA-2 Chat (7B)","display_type":"chat","description":"Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-7b-chat","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"6738415616","finetuning_supported":true,"context_length":4096,"config":{"prompt_format":"[INST] {prompt} [/INST]","stop":["[/INST]","</s>"]},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/llama-2-7b","display_name":"LLaMA-2 (7B)","display_type":"language","description":"Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters","license":"LLaMA license Agreement (Meta)","link":"https://huggingface.co/togethercomputer/llama-2-7b","creator_organization":"Meta","pricing_tier":"Featured","num_parameters":"6738415616","finetuning_supported":true,"context_length":4096,"config":{},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/mpt-30b-instruct","display_name":"MPT-Instruct (30B)","display_type":"language","description":"Designed for short-form instruction following, finetuned on Dolly and Anthropic HH-RLHF and other datasets","license":"CC-By-SA-3.0","link":"https://huggingface.co/mosaicml/mpt-30b-instruct","creator_organization":"Mosaic ML","pricing_tier":"supported","num_parameters":30000000000,"context_length":2048,"config":{"prompt_format":"### Instruction:\n{prompt}\n### Response:\n","stop":["<|endoftext|>","###"]},"pricing":{"input":200,"output":200,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/mpt-30b","display_name":"MPT (30B)","display_type":"language","description":"Decoder-style transformer pretrained from scratch on 1T tokens of English text and code.","license":"apache-2.0","link":"https://huggingface.co/mosaicml/mpt-30b","creator_organization":"Mosaic ML","pricing_tier":"supported","num_parameters":30000000000,"context_length":2048,"config":{"stop":["<|endoftext|>"]},"pricing":{"input":200,"output":200,"hourly":0},"descriptionLink":""},{"name":"togethercomputer/mpt-7b-chat","display_name":"MPT-Chat (7B)","display_type":"chat","description":"Chat model for dialogue generation finetuned on ShareGPT-Vicuna, Camel-AI, GPTeacher, Guanaco, Baize and some generated datasets.","license":"cc-by-nc-sa-4.0","link":"https://huggingface.co/mosaicml/mpt-7b-chat","creator_organization":"Mosaic ML","pricing_tier":"supported","num_parameters":7000000000,"context_length":2048,"config":{"stop":["<|im_end|>"],"prompt_format":"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"},"pricing":{"input":50,"output":50,"hourly":0},"descriptionLink":""},{"name":"upstage/SOLAR-0-70b-16bit","display_name":"SOLAR v0 (70B)","display_type":"chat","description":"Language model instruction fine-tuned by upstage.ai on Orca and Alpaca style datasets that reached the top spot in openLLM rankings","license":"CC BY-NC-4.0","creator_organization":"Upstage","num_parameters":70000000000,"release_date":"2023-08-01T00:00:00.000Z","context_length":4096,"config":{"stop":["###"],"prompt_format":"### System:\nYou are a respectful and helpful assistant.\n### User:\n{prompt}\n### Assistant:"},"pricing":{"input":250,"output":250,"hourly":0},"link":"","descriptionLink":""},{"name":"wavymulder/Analog-Diffusion","display_name":"Analog Diffusion","display_type":"image","description":"Dreambooth model trained on a diverse set of analog photographs to provide an analog film effect. ","license":"creativeml-openrail-m","link":"https://huggingface.co/wavymulder/Analog-Diffusion","creator_organization":"Wavymulder","pricing_tier":"supported","num_parameters":0,"external_pricing_url":"https://www.together.xyz/apis#pricing","descriptionLink":"","pricing":{"hourly":0,"input":0,"output":0,"base":0,"finetune":0}}]